{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('ahihi/labels')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "save_dir = Path(\"ahihi\")\n",
    "save_txt = \"ahaha\"\n",
    "\n",
    "save_dir / 'labels' if save_txt else save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights_speed=['./weights/traffic_speed_sign.pt'], weights_distance='weights/yolov7.pt', source='videos/car3.mp4', image_size=1024, conf_thres=0.25, iou_thres=0.45, device='', view_image=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TRAN DAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\Computer Engineering Project\\Deploy\\system_inference.py\", line 185, in <module>\n",
      "    main(opt)\n",
      "  File \"e:\\Computer Engineering Project\\Deploy\\system_inference.py\", line 159, in main\n",
      "    dir_path, vid_path = inference_in_files(source, save_dir, image_size, stride, opt, device, model_speed, names, colors)\n",
      "  File \"e:\\Computer Engineering Project\\Deploy\\system_inference.py\", line 47, in inference_in_files\n",
      "    pred = model_speed(img, augment=opt.augment)[0]\n",
      "  File \"c:\\Users\\TRAN DAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"e:\\Computer Engineering Project\\Deploy\\utils\\torch_utils.py\", line 213, in forward\n",
      "    out = self.model(x)\n",
      "  File \"c:\\Users\\TRAN DAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "RuntimeError: The following operation failed in the TorchScript interpreter.\n",
      "Traceback of TorchScript (most recent call last):\n",
      "c:\\Users\\TRAN DAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py(458): _conv_forward\n",
      "c:\\Users\\TRAN DAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py(463): forward\n",
      "c:\\Users\\TRAN DAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py(1172): _slow_forward\n",
      "c:\\Users\\TRAN DAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py(1188): _call_impl\n",
      "e:\\Computer Engineering Project\\Deploy\\models\\common.py(111): fuseforward\n",
      "c:\\Users\\TRAN DAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py(1172): _slow_forward\n",
      "c:\\Users\\TRAN DAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py(1188): _call_impl\n",
      "e:\\Computer Engineering Project\\Deploy\\models\\yolo.py(613): forward_once\n",
      "e:\\Computer Engineering Project\\Deploy\\models\\yolo.py(588): forward\n",
      "c:\\Users\\TRAN DAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py(1172): _slow_forward\n",
      "c:\\Users\\TRAN DAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py(1188): _call_impl\n",
      "c:\\Users\\TRAN DAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\jit\\_trace.py(957): trace_module\n",
      "c:\\Users\\TRAN DAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\jit\\_trace.py(753): trace\n",
      "e:\\Computer Engineering Project\\Deploy\\utils\\torch_utils.py(195): __init__\n",
      "e:\\Computer Engineering Project\\Deploy\\system_inference.py(125): main\n",
      "e:\\Computer Engineering Project\\Deploy\\system_inference.py(168): <module>\n",
      "RuntimeError: \"slow_conv2d_cpu\" not implemented for 'Half'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights\\traffic_speed_sign.pt\n",
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n",
      "Model Summary: 314 layers, 36535712 parameters, 6194944 gradients, 103.3 GFLOPS\n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "video 1/1 (1/304) e:\\Computer Engineering Project\\Deploy\\videos\\car3.mp4: \n"
     ]
    }
   ],
   "source": [
    "!python system_inference.py --weights_speed ./weights/traffic_speed_sign.pt --conf 0.25 --image-size 1024 --source videos/car3.mp4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "236d1ceb6bd37800f90b7122e0844be2021ff36742422b7e7db49d34370ac8a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
